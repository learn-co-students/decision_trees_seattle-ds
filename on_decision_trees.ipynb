{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import log\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a simple internet image search for decision trees.\n",
    "\n",
    "What does this have to do with a data science predictive model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans.**: Predictions of the behavior of the dependent variable are dependent on answers to questions about the values of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a machine learning model that works by *partitioning* our sample space in a hierarchical way.\n",
    "\n",
    "*How* do we partition the space? The key idea is that some attributes provide more *information* than others when trying to make a decision.\n",
    "\n",
    "## Motivating Example\n",
    "\n",
    "Suppose a friend said to you: \"Hey, you have experience predicting housing prices! I'm hoping you can predict what my house will sell for.\"\n",
    "\n",
    "What are the top questions you would ask about your friend's house? Let's make a list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how many bedrooms does it have?\n",
    "- what is the square footage?\n",
    "- what sort of view does it have?\n",
    "- what's its ZIP code?\n",
    "- is there high traffic?\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now: *Why* are these questions good questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans.**: The answers to these questions have a great effect on the selling price that we should predict. The color of paint would likely make very little difference to how much the house would sell for, but its number of bedrooms (etc.) would likely make a very significant difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Bit More Detail\n",
    "\n",
    "Suppose that we work for a retail company and that we're working on a classification model that can predict whether customers will pay their bills or not. In effect what we want to do is to ask questions that will *sort* our customers into two classes: (i) those who pay and (ii) those who don't.\n",
    "\n",
    "(Can you think of any ethical issues that could arise here?)\n",
    "\n",
    "**If I ask enough data-dividing questions, I can produce a partition of my data such that, in each subset, either (i) everyone is a payer or (ii) everyone is a non-payer.**\n",
    "\n",
    "Each row in my dataframe represents a customer, and I have many predictors (columns) in my dataframe, including:\n",
    "\n",
    "- salary\n",
    "- total_bill\n",
    "- club_member (boolean)\n",
    "- age\n",
    "\n",
    "Let's look at a simple set of data. **The 'paid' column is our target or dependent variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custs = pd.DataFrame([[45000, 1000, True, 20, False],\n",
    "                      [70000, 100, True, 50, True],\n",
    "             [30000, 2000, False, 25, False],\n",
    "                      [90000, 500, True, 44, True],\n",
    "             [70000, 200, True, 48, False]],\n",
    "            columns=['salary', 'total_bill', 'club_member',\n",
    "                     'age', 'paid'])\n",
    "custs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "I partition my data by asking a question about the independent variables. The goal is to ask the right questions in the right order so that the resultant groups are \"pure\" with respect to the dependent variable. More on this below!\n",
    "\n",
    "Suppose, for example, that I choose:\n",
    "\n",
    "### Is the customer a club member?\n",
    "\n",
    "This would divide my data into two groups:\n",
    "\n",
    "- Group 1 (club_member = True):\n",
    "\n",
    "data points: 0, 1, 3, 4\n",
    "\n",
    "- Group 2 (club_member = False):\n",
    "\n",
    "data points: 2\n",
    "\n",
    "#### Key Question: How are the values of the target distributed in this group?\n",
    "In Group 1, I have, in order: non-payer, payer, payer, non-payer.\n",
    "\n",
    "In Group 2, I have a single non-payer.\n",
    "\n",
    "While I've isolated one of the customers who haven't paid in the second group, the first group is an even mix of payers and non-payers. So this split is not particularly good.\n",
    "\n",
    "Would a different question split our data more effectively? Let's try:\n",
    "\n",
    "### \"Is the customer's salary less than $60k?\"\n",
    "\n",
    "This would divide my data into two groups:\n",
    "\n",
    "- Group 1 (salary < 60000):\n",
    "\n",
    "data points: 0, 2\n",
    "\n",
    "-  Group 2 (salary $\\geq$ 60000):\n",
    "\n",
    "data points: 1, 3, 4\n",
    "\n",
    "#### Key Question: How are the values of the target distributed in this group?\n",
    "In Group 1, I have two non-payers.\n",
    "\n",
    "In Group 2, I have, in order: payer, payer, non-payer.\n",
    "\n",
    "This does a better job of partitioning my data according to the values of the dependent variable: The first group contains only customers who have not paid their bills, and the second group contains only one customer who has not paid her bill.\n",
    "\n",
    "So a (very simple!) model that predicts:\n",
    "(i) that customers who make less than \\$60k *won't* pay their bill, and\n",
    "(ii) that customers who make \\$60k or more *will* pay their bill\n",
    "\n",
    "would perform fairly well.\n",
    "\n",
    "But how would my partition be *best* split? And how do I really know that the second split is better than the first? Can I do better than intuition here?\n",
    "\n",
    "## Entropy and Information Gain\n",
    "\n",
    "The goal is to have our ultimate classes be fully \"ordered\" (for a binary dependent variable, we'd have the 1's in one group and the 0's in the other). So one way to assess the value of a split is to measure how *disordered* our groups are, and there is a notion of *entropy* that measures precisely this.\n",
    "\n",
    "The entropy of the whole dataset is given by:\n",
    "\n",
    "$\\large E = -\\Sigma^n_i p_i\\log_2(p_i)$,\n",
    "\n",
    "where $p_i$ is the probability of belonging to the $i$th group, where $n$ is the number of groups (i.e. target values).\n",
    "\n",
    "**Entropy will always be between 0 and 1. The closer to 1, the more disordered your group.**\n",
    "\n",
    "To repeat, in the present case we have only two groups of interest: the payers and the non-payers.\n",
    "\n",
    "Two out of five are payers and three out of five are non-payers, so **these are the relevant probabilities** for our calculation of entropy.\n",
    "\n",
    "So our entropy for this toy dataset is:\n",
    "\n",
    "$-0.4*\\log_2(0.4) -0.6*\\log_2(0.6)$.\n",
    "\n",
    "Let's use the ```math``` library's `log()` function to calculate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n",
    "-0.4 * log(0.4, 2) - 0.6 * log(0.6, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty disordered!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the entropy of a *split*, we're going to want to calculate the entropy of each of the groups made by the split, and then calculate a weighted average of those groups' entropies––weighted, that is, by the size of the groups. Let's calculate the entropy of the split produced by our question above about salary:\n",
    "\n",
    "Group 1:\n",
    "\n",
    "$E_{g1} = - 1 * \\log_2(1) = 0$. This is a pure group! The probability of being a payer in Group 1 is 0 and the probability of being a non-payer in Group 1 is 1.\n",
    "\n",
    "Group 2:\n",
    "\n",
    "$E_{g2} = -\\frac{2}{3} * \\log_2\\left(\\frac{2}{3}\\right) - \\frac{1}{3} * \\log_2\\left(\\frac{1}{3}\\right)$.\n",
    "\n",
    "Once again, using ```math```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n",
    "ent_grp1 = 0\n",
    "ent_grp2 = -2/3 * log(2/3, 2) - 1/3 * log(1/3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the whole entropy for this split, we'll do a weighted sum of the two group entropies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n",
    "0.4 * ent_grp1 + 0.6 * ent_grp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given split, the **information gain** is simply the entropy of the parent group less the entropy of the split.\n",
    "\n",
    "For a given parent, then, we maximize our model's performance by *minimizing* the split's entropy.\n",
    "\n",
    "What we'd like to do then is:\n",
    "\n",
    "1. to look at the entropies of all possible splits, and\n",
    "2. to choose the split with the lowest entropy.\n",
    "\n",
    "In practice there are far too many splits for it to be practical for a person to calculate all these different entropies ...\n",
    "\n",
    "... but we can make computers do these calculations for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Impurity\n",
    "\n",
    "An alternative metric to entropy comes from the work of Corrado Gini. The Gini Impurity is defined as:\n",
    "\n",
    "$\\large G = 1 - \\Sigma_ip_i^2$, or, equivalently, $\\large G = \\Sigma_ip_i(1-p_i)$.\n",
    "\n",
    "where, again, $p_i$ is the probability of belonging to the $i$th group.\n",
    "\n",
    "**Gini Impurity will always be between 0 and 0.5. The closer to 0.5, the more disordered your group.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Calculate the Gini Impurity for our toy dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n",
    "1 - (0.6**2 + 0.4**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Trees in Python\n",
    "\n",
    "Scikit-learn has a tree module, which houses both a DecisionTreeClassifier and a DecisionTreeRegressor. As is probably clear by now, the former is for classification problems (discrete target) and the latter is for regression problems (continuous target). Let's use the classifier.\n",
    "\n",
    "(How does the regressor work? Information gain and Gini impurity make sense only for discrete classes. What we need for a continuous target is some measure of the *spread* of those values, and so the natural move here is to split our groups in ways that will most *reduce the standard deviation of those groups*. For more on this, see [here](https://www.saedsayad.com/decision_tree_reg.htm).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = DecisionTreeClassifier(criterion='entropy', random_state=1)    # Check out all the\n",
    "                                                                    # hyperparameter options here!\n",
    "\n",
    "ct.fit(custs.drop('paid', axis=1), custs['paid'])\n",
    "ct.score(custs.drop('paid', axis=1), custs['paid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = ct.tree_.node_count\n",
    "children_left = ct.tree_.children_left\n",
    "children_right = ct.tree_.children_right\n",
    "feature = ct.tree_.feature\n",
    "threshold = ct.tree_.threshold\n",
    "\n",
    "# This code courtesy of sklearn:\n",
    "# https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
    "\n",
    "\n",
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\"The binary tree structure has %s nodes and has \"\n",
    "      \"the following tree structure:\"\n",
    "      % n_nodes)\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many nodes do we have?\n",
    "\n",
    "ct.tree_.node_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells us which node is immediately to the left of each node in our tree.\n",
    "\n",
    "ct.tree_.children_left\n",
    "\n",
    "# This tells us which node is immediately to the right of each node in our tree.\n",
    "\n",
    "ct.tree_.children_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "Let's unpack this:\n",
    "\n",
    "- The first question (\"test\") is: \"Is Age $\\leq$ 34.5?\" If so, then stop: All such customers (i.e. customers 0 and 2) are *non-payers*. Notice how 34.5 is midway between the ages of customer 2 (25), a non-payer and customer 3 (44), a payer).\n",
    "\n",
    "- If not: The next test (for those whose age > 34.5) is: \"Is Salary $\\leq$ 80000?\" If not, then stop: All such customers (i.e. customer 3) are *payers*. Notice how 80000 is midway between the salaries of customer 3 (90000, a payer) and customer 4 (70000, a non-payer).\n",
    "\n",
    "- If so: The next test (for those whose salary $\\leq$ 80000) is: \"Is Age $\\leq$ 49.0?\" If so, then stop: All such customers (i.e. customer 4) are *non-payers*. If not, then stop: All such customers (i.e. customer 1) are *payers*. Notice how 49.0 is midway between the ages of customer 1 (50, a payer) and customer 4 (48, a non-payer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances\n",
    "\n",
    "The fitted tree has an attribute called `ct.feature_importances_`. What does this mean? Roughly, the importance (or \"Gini importance\") of a feature is a sort of weighted average of the impurity decrease at internal nodes that make use of the feature. The weighting comes from the number of samples that depend on the relevant nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can calculate these by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropies at Internal Nodes:\n",
    "n0_ent = -(0.6 * log(0.6, 2) + 0.4 * log(0.4, 2))\n",
    "n2_ent = -((2/3) * log((2/3), 2) + (1/3) * log((1/3), 2))\n",
    "n3_ent = -(0.5 * log(0.5, 2) + 0.5 * log(0.5, 2))\n",
    "\n",
    "# Node Importances\n",
    "n0_imp = n0_ent - 0.6 * n2_ent\n",
    "n2_imp = 0.6 * n2_ent - 0.4 * n3_ent\n",
    "n3_imp = 0.4 * n3_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_imp_sum = n0_imp + n2_imp + n3_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2_imp / n_imp_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(n0_imp + n3_imp) / n_imp_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3) for more on feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice on a Larger Dataset\n",
    "\n",
    "Exercise: Use a tree for prediction. Using the data below, use height and weight to predict gender.\n",
    "\n",
    "Make a visualization or two, and write a short paragraph about your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts_hts = pd.read_csv('weight-height.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts_hts['Gen_bool'] = wts_hts['Gender'].map(lambda x: 1 if x == 'Male' else 0)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(wts_hts['Height'], wts_hts['Weight'], c=wts_hts['Gen_bool'])\n",
    "ax.set_xlabel('height')\n",
    "ax.set_ylabel('weight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "X = wts_hts.drop('Gender', axis=1)\n",
    "y = wts_hts['Gender']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bool = y_train.map(lambda x: 1 if x == 'Male' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(tree, X_train, y=y_bool, scoring='precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(tree, X_train, y=y_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
